#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Sep  7 15:48:41 2018

@author: cdz
"""

import os
import tensorflow as tf
import numpy as np
import math
import timeit
import matplotlib.pyplot as plt



# set some global variables
USE_GPU=False
if USE_GPU:
    device='/device:GPU:0'
else:
    device='/cpu:0'

print_every=100



########################## part1: preparation
def load_cifar10(num_training=49000,num_validation=1000,num_test=10000):
    """
    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare
    it for the two-layer neural net classifier. These are the same steps as
    we used for the SVM, but condensed to a single function.
    """
    # load the raw CIFAR-10 dataset
    cifar10=tf.keras.datasets.cifar10.load_data()
    (X_train,y_train),(X_test,y_test)=cifar10
    X_train=np.asarray(X_train,dtype=np.float32)
    y_train=np.asarray(y_train,dtype=np.int32).flatten()
    X_test=np.asarray(X_test,dtype=np.float32)
    y_test=np.asarray(y_test,dtype=np.int32).flatten()
    
    # subsample the data
    mask=range(num_training,num_training+num_validation)
    X_val=  X_train[mask]
    y_val=y_train[mask]
    
    mask=range(num_training)
    X_train=X_train[mask]
    y_train=y_train[mask]
    
    mask=range(num_test)
    X_test=X_test[mask]
    y_test=y_test[mask]
    
    # normalize the data: subtract the mean pixel and divide by std
    # std means: standard deviation
    mean_pixel=X_train.mean(axis=(0,1,2),keepdims=True) # note:axis=(0,1,2) means that sum all numbers in dimesion(0,1,2),and then divide (shape[0]+sahpe[1]+shape[2])                
    std_pixel=X_train.std(axis=(0,1,2),keepdims=True)
    X_train=(X_train-mean_pixel)/std_pixel
    X_val=(X_val-mean_pixel)/std_pixel
    X_test=(X_test-mean_pixel)/std_pixel
    
    return X_train,y_train,X_val,y_val,X_test,y_test



# i can't understand!!!!!
class Dataset(object):
    
    def __init__(self,X,y,batch_size,shuffle=False):
        '''
         Construct a Dataset object to iterate over data X and labels y
        
        Inputs:
        - X: Numpy array of data, of any shape
        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]
        - batch_size: Integer giving number of elements per minibatch
        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch
        '''
        assert X.shape[0]==y.shape[0], "got different numbers of data and labels"
        self.X,self.y=X,y
        self.batch_size,self.shuffle=batch_size,shuffle
    
    def __iter__(self):
        N,B=self.X.shape[0],self.batch_size
        index=np.arange(N)
        if self.shuffle:  # disrupt the index
            np.random.shuffle(index) 
        return iter((self.X[i:i+B],self.y[i:i+B]) for i in range(0,N,B))


# one question: where is the downloaded dataset????
# note: use(0,1,2) to get mean
# Invoke the above function to get our data.
NHW = (0, 1, 2)
X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()
#print('Train data shape: ', X_train.shape)
#print('Train labels shape: ', y_train.shape, y_train.dtype)
#print('Validation data shape: ', X_val.shape)
#print('Validation labels shape: ', y_val.shape)
#print('Test data shape: ', X_test.shape)
#print('Test labels shape: ', y_test.shape)


# preparation : dataset object 
train_dset=Dataset(X_train,y_train,batch_size=64,shuffle=True)
val_dset=Dataset(X_val,y_val,batch_size=64,shuffle=False)
test_dset=Dataset(X_test,y_test,batch_size=64)

########################################  part2 : barebone tensorflow
# flatten function
def flatten(x):
    '''
    intput:
        tensorflow tensor of shape(N,D1,.....Dm)
    output:
        tensorflow trensor of shape(N,D1*.....*Dm)
    '''
    N=tf.shape(x)[0]
    return tf.reshape(x,(N,-1))

def test_flatten():
    # clear the current tewnsorflow graph
    tf.reset_default_graph()
    
    # Stage I: Define the TensorFlow graph describing our computation.
    # In this case the computation is trivial: we just want to flatten
    # a Tensor using the flatten function defined above.
    
    # Our computation will have a single input, x. We don't know its
    # value yet, so we define a placeholder which will hold the value
    # when the graph is run. We then pass this placeholder Tensor to
    # the flatten function; this gives us a new Tensor which will hold
    # a flattened view of x when the graph is run. The tf.device
    # context manager tells TensorFlow whether to place these Tensors
    # on CPU or GPU.
    with tf.device(device):
        x=tf.placeholder(tf.float32) # placeholder means the inputs to the computational graph
        x_flat=flatten(x)
        
    # At this point we have just built the graph describing our computation,
    # but we haven't actually computed anything yet. If we print x and x_flat
    # we see that they don't hold any data; they are just TensorFlow Tensors
    # representing values that will be computed when the graph is run.
    print('x: ',type(x),x)
    print('x_flat:',type(x_flat),x_flat)
    print()
    
    # We need to use a TensorFlow Session object to actually run the graph.
    with tf.Session() as sess:
        # construct the concrret values of the input data x, using numpy
        
        x_np=np.arange(24).reshape((2,3,4))
        print('x_np:\n',x_np,'\n')
        # Run our computational graph to compute a concrete output value.
        # The first argument to sess.run tells TensorFlow which Tensor
        # we want it to compute the value of;  the feed_dict specifies
        # values to plug into all placeholder nodes in the graph. The
        # resulting value of x_flat is returned from sess.run as a
        # numpy array. 
        x_flat_np=sess.run(x_flat,feed_dict={x:x_np})
        print('x_flat_np:\n', x_flat_np, '\n')
         
        # we can reuse the same graph to perform the same computation with different input data
        x_np=np.arange(12).reshape((2,3,2))
        print('x_np: ',x_np,'\n')
        x_flat_np=sess.run(x_flat,feed_dict={x:x_np})
        print('x_flat_np',x_flat_np)

# barebones tensorflow: two-layer network
def two_layer_fc(x,params):
    # in this function, we don't caculate the bias
    '''
    A fully-connected neural network; the architecture is:
    fully-connected layer -> ReLU -> fully connected layer.
    Note that we only need to define the forward pass here; TensorFlow will take
    care of computing the gradients for us.
    
    The input to the network will be a minibatch of data, of shape
    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,
    and the output layer will produce scores for C classes.

    Inputs:
    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of
      input data.
    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the
      network, where w1 has shape (D, H) and w2 has shape (H, C).
    
    Returns:
    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores
      for the input data x.
    '''
    w1,w2=params 
    x=flatten(x) # now the shape of x is (N,D)
    h=tf.nn.relu(tf.matmul(x,w1)) #hidden layer (N,H)
    scores=tf.matmul(h,w2)  # of shape(N,C)
    return scores
    
def two_layer_fc_test():
    '''
    attention: feed_dict={x:x_np} means that we use concret x_np to assignment x 
    '''
    # TensorFlow's default computational graph is essentially a hidden global
    # variable. To avoid adding to this default graph when you rerun this cell,
    # we clear the default graph before constructing the graph we care about.
    tf.reset_default_graph()
    hidden_layer_size=42
    
    # Scoping our computational graph setup code under a tf.device context
    # manager lets us tell TensorFlow where we want these Tensors to be
    # placed.
    with tf.device(device):
         # Set up a placehoder for the input of the network, and constant
        # zero Tensors for the network weights. Here we declare w1 and w2
        # using tf.zeros instead of tf.placeholder as we've seen before - this
        # means that the values of w1 and w2 will be stored in the computational
        # graph itself and will persist across multiple runs of the graph; in
        # particular this means that we don't have to pass values for w1 and w2
        # using a feed_dict when we eventually run the graph.
        x=tf.placeholder(tf.float32)
        w1=tf.zeros((32*32*3,hidden_layer_size))
        w2=tf.zeros((hidden_layer_size,10))
        
        # Call our two_layer_fc function to set up the computational
        # graph for the forward pass of the network.
        scores=two_layer_fc(x,[w1,w2])
        
    # Use numpy to create some concrete data that we will pass to the
    # computational graph for the x placeholder.
    x_np=np.zeros((64,32,32,3))
    with tf.Session() as sess:
        # The calls to tf.zeros above do not actually instantiate the values
        # for w1 and w2; the following line tells TensorFlow to instantiate
        # the values of all Tensors (like w1 and w2) that live in the graph.
        sess.run(tf.global_variables_initializer())
        
         # Here we actually run the graph, using the feed_dict to pass the
        # value to bind to the placeholder for x; we ask TensorFlow to compute
        # the value of the scores Tensor, which it returns as a numpy array.
        scores_np=sess.run(scores,feed_dict={x:x_np})
        print('scores_np:',scores_np.shape)

#......
def three_layer_convnet(x,params):
    '''
    A three-layer convolutional network with the architecture described above.
    
    Inputs:
    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images
    - params: A list of TensorFlow Tensors giving the weights and biases for the
      network; should contain the following:
      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving
        weights for the first convolutional layer.
      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the
        first convolutional layer.
      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)
        giving weights for the second convolutional layer
      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the
        second convolutional layer.
      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.
        Can you figure out what the shape should be?
      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.
        Can you figure out what the shape should be?
    
    tf.pad(
    tensor,
    paddings,
    mode='CONSTANT',
    name=None,
    constant_values=0
    )
    
    tf.nn.conv2d(
    input,
    filter,
    strides,
    padding,
    use_cudnn_on_gpu=True,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
     )
    '''
    
    conv_w1,conv_b1,conv_w2,conv_b2,fc_w,fc_b=params
  
    x_pad=tf.pad(x,[[0,0],[2,2],[2,2],[0,0]],'CONSTANT') # [2,2] means both ends of that dimension padding 2 elements(0)
    conv1=tf.nn.conv2d(x_pad,conv_w1,[1,1,1,1],padding='VALID')+conv_b1
    relu1=tf.nn.relu(conv1)
    
    relu1_pad=tf.pad(relu1,[[0,0],[1,1],[1,1],[0,0]],'CONSTANT')
    conv2=tf.nn.conv2d(relu1_pad,conv_w2,[1,1,1,1],padding='VALID')+conv_b2
    relu2=tf.nn.relu(conv2)
    
    conv2_flat=flatten(relu2)
    scores=tf.matmul(conv2_flat,fc_w)+fc_b
    
    return scores
    
def three_layers_convnet_test():
    tf.reset_default_graph()
    
    with tf.device(device):
        x=tf.placeholder(tf.float32)
        conv_w1=tf.zeros((5,5,3,6)) # 5*5*3 (six filters)
        conv_b1=tf.zeros((6,))
        conv_w2=tf.zeros((3,3,6,9)) # 3*3*6( nine filters), 6 depends on filters in last layer
        conv_b2=tf.zeros((9,))
        fc_w=tf.zeros((32*32*9,10))
        fc_b=tf.zeros((10,))
        params=[conv_w1,conv_b1,conv_w2,conv_b2,fc_w,fc_b]
        scores=three_layer_convnet(x,params)
    
    # Inputs to convolutional layers are 4-dimensional arrays with shape
    # [batch_size, height, width, channels]
    x_np=np.zeros((64,32,32,3))
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer()) # this step to initialize the variables
        scores_np=sess.run(scores,feed_dict={x:x_np})
        print('scores_np shapoe: ',scores_np.shape)

#....training step
def training_step(scores,y,params,learning_rate):
    '''
     Set up the part of the computational graph which makes a training step.

    Inputs:
    - scores: TensorFlow Tensor of shape (N, C) giving classification scores for
      the model.
    - y: TensorFlow Tensor of shape (N,) giving ground-truth labels for scores;
      y[i] == c means that c is the correct class for scores[i].
    - params: List of TensorFlow Tensors giving the weights of the model
    - learning_rate: Python scalar giving the learning rate to use for gradient
      descent step.
      
    Returns:
    - loss: A TensorFlow Tensor of shape () (scalar) giving the loss for this
      batch of data; evaluating the loss also performs a gradient descent step
      on params (see above).
    '''
    # First compute the loss; the first line gives losses for each example in
    # the minibatch, and the second averages the losses acros the batch
    losses=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=scores)
    loss=tf.reduce_mean(losses)
    
    # Compute the gradient of the loss with respect to each parameter of the the
    # network. This is a very magical function call: TensorFlow internally
    # traverses the computational graph starting at loss backward to each element
    # of params, and uses backpropagation to figure out how to compute gradients;
    # it then adds new operations to the computational graph which compute the
    # requested gradients, and returns a list of TensorFlow Tensors that will
    # contain the requested gradients when evaluated.
    grad_params=tf.gradients(loss,params)
    
    # make a gradient descent step on  all of the model parameters
    new_weights=[]
    for w,grad_w in zip(params,grad_params):
        new_w=tf.assign_sub(w,learning_rate*grad_w)
        new_weights.append(new_w)
    
    # Insert a control dependency so that evaluting the loss causes a weight
    # update to happen; see the discussion above.
    with tf.control_dependencies(new_weights):
        return tf.identity(loss) #tf.identity是返回了一个一模一样新的tensor，再control_dependencies的作用块下，需要增加一个新节点到gragh中。
    
    
#....training loop
def train_part2(model_fn,init_fn,learning_rate):
    '''
     Train a model on CIFAR-10.
    
    Inputs:
    - model_fn: A Python function that performs the forward pass of the model
      using TensorFlow; it should have the following signature:
      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a
      minibatch of image data, params is a list of TensorFlow Tensors holding
      the model weights, and scores is a TensorFlow Tensor of shape (N, C)
      giving scores for all elements of x.
    - init_fn: A Python function that initializes the parameters of the model.
      It should have the signature params = init_fn() where params is a list
      of TensorFlow Tensors holding the (randomly initialized) weights of the
      model.
    - learning_rate: Python float giving the learning rate to use for SGD.
    '''
    # first clear tye default graph
    tf.reset_default_graph()
    is_training=tf.placeholder(tf.bool,name='is_training')
    
    # Set up the computational graph for performing forward and backward passes,
    # and weight updates.
    with tf.device(device):
        # set up placeholders for the data and labels
        x=tf.placeholder(tf.float32,[None,32,32,3])
        y=tf.placeholder(tf.int32,[None])
        params=init_fn()           # initialize the model parameters
        scores=model_fn(x,params)  # forward pass of the model
        loss=training_step(scores,y,params,learning_rate)
    
    # Now we actually run the graph many times using the training data
    with tf.Session() as sess:
        # intialize variablles that will live in the graph
        sess.run(tf.global_variables_initializer())
        
        for t,(x_np,y_np) in enumerate(train_dset):
            # Run the graph on a batch of training data; recall that asking
            # TensorFlow to evaluate loss will cause an SGD step to happen.
            feed_dict={x:x_np,y:y_np}
            loss_np=sess.run(loss,feed_dict=feed_dict)
            
            # Periodically print the loss and check accuracy on the val set
            if t%print_every==0:
                print('Iteration %d, loss = %.4f' % (t, loss_np))
                check_accuracy(sess,val_dset,x,scores,is_training)


def check_accuracy(sess,dset,x,scores,is_training=None):
    '''
     Check accuracy on a classification model.
    
    Inputs:
    - sess: A TensorFlow Session that will be used to run the graph
    - dset: A Dataset object on which to check accuracy
    - x: A TensorFlow placeholder Tensor where input images should be fed
    - scores: A TensorFlow Tensor representing the scores output from the
      model; this is the Tensor we will ask TensorFlow to evaluate.
      
    Returns: Nothing, but prints the accuracy of the model
    '''
    num_correct,num_samples=0,0
    for x_batch,y_batch in dset:
        feed_dict={x:x_batch,is_training:0}
        scores_np=sess.run(scores,feed_dict=feed_dict)
        y_pred=scores_np.argmax(axis=1)
        num_samples+=x_batch.shape[0]
        num_correct+=(y_pred==y_batch).sum()
    acc=float(num_correct)/num_samples
    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))

#...initialization
def kaiming_normal(shape):
    if len(shape)==2:
        fan_in,fan_out=shape[0],shape[1]
    elif len(shape)==4:
        fan_in,fan_out=np.prod(shape[:3]),shape[3]
    return tf.random_normal(shape)*np.sqrt(2.0/fan_in)


def two_layer_fc_init():
    '''
    Initialize the weights of a two-layer network, for use with the
    two_layer_network function defined above.
    
    Inputs: None
    
    Returns: A list of:
    - w1: TensorFlow Variable giving the weights for the first layer
    - w2: TensorFlow Variable giving the weights for the second layer
    '''
    hidden_layer_size=4000
    w1=tf.Variable(kaiming_normal((3*32*32,4000)))
    w2=tf.Variable(kaiming_normal((4000,10)))
    return [w1,w2]
'''
learning_rate=1e-2
train_part2(two_layer_fc,two_layer_fc_init,learning_rate)
'''
def three_layer_convnet_init():
    '''
     Initialize the weights of a Three-Layer ConvNet, for use with the
    three_layer_convnet function defined above.
    
    Inputs: None
    
    Returns a list containing:
    - conv_w1: TensorFlow Variable giving weights for the first conv layer
    - conv_b1: TensorFlow Variable giving biases for the first conv layer
    - conv_w2: TensorFlow Variable giving weights for the second conv layer
    - conv_b2: TensorFlow Variable giving biases for the second conv layer
    - fc_w: TensorFlow Variable giving weights for the fully-connected layer
    - fc_b: TensorFlow Variable giving biases for the fully-connected layer
    '''
    conv_w1=tf.Variable(kaiming_normal((5,5,3,32)))
    conv_b1=tf.Variable(tf.zeros([32]))
    
    conv_w2=tf.Variable(kaiming_normal((3,3,32,16)))
    conv_b2=tf.Variable(tf.zeros([16]))
    
    fc_w=tf.Variable(kaiming_normal((32*32*16,10)))
    fc_b=tf.Variable(tf.zeros([10]))
    
    params=[conv_w1,conv_b1,conv_w2,conv_b2,fc_w,fc_b]
    
    return params
'''
learning_rate=3e-3
train_part2(three_layer_convnet,three_layer_convnet_init,learning_rate)
'''


#######################################################  part3 Keras Model API
#####################################################
class TwoLayerFC(tf.kertas.Model):
    def __init__():



























#......................................................   test part ....................
'''
# one question: where is the downloaded dataset????
# note: use(0,1,2) to get mean
# Invoke the above function to get our data.
NHW = (0, 1, 2)
X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()
#print('Train data shape: ', X_train.shape)
#print('Train labels shape: ', y_train.shape, y_train.dtype)
#print('Validation data shape: ', X_val.shape)
#print('Validation labels shape: ', y_val.shape)
#print('Test data shape: ', X_test.shape)
#print('Test labels shape: ', y_test.shape)


# preparation : dataset object 
train_dset=Dataset(X_train,y_train,batch_size=64,shuffle=True)
val_dset=Dataset(X_val,y_val,batch_size=64,shuffle=False)
test_dset=Dataset(X_test,y_test,batch_size=64)
for t,(x,y) in enumerate(val_dset):
    print(t,x.shape,y.shape)
    if t>5: break

test_flatten()
   
two_layer_fc_test()    
'''

with tf.device('/cpu:0'):    
    three_layers_convnet_test()

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    